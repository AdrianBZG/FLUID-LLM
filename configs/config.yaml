batch_size: 8
checkpoint_save_path: model_checkpoints
decoder_params:
  activation: relu
  hidden_dim: 512
  num_layers: 2
  type: MLP
enable_wandb: true
encoder_params:
  activation: relu
  hidden_dim: 512
  num_layers: 2
  type: MLP
epoch_size: 100
flash_attention: true
freeze_llm: false
half_precision: true
input_emb_layer_norm_eps: 1.0e-05
learning_rate: 0.0007
llm_4bit_loading: false
llm_backbone: facebook/opt-125m
llm_layers: 7
load_dir: ./ds/MGN/cylinder_dataset
lora_config:
  bias: none
  lora_alpha: 64
  lora_dropout: 0.1
  r: 16
loss_function:
- mae
- mse
loss_weighting:
- 0.3
- 10
multiprocess: true
num_epochs: 200
num_workers: 8
optimizer: adam
patch_size:
- 16
- 16
pos_embedding_type: pos
resolution: 240
save_model_each: 10
seq_interval: 2
seq_len: 10
stride:
- 16
- 16
task_name: cylinder_task
use_patches_self_attention: false
weight_decay: 0
