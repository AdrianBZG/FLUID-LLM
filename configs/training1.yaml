task_name: cylinder_task

# LLM params
llm_backbone: huggyllama/llama-7b
llm_layers: 32
llm_4bit_loading: false
freeze_llm: false
lora_config:
  r: 8
  lora_alpha: 16
  lora_dropout: 0.1
  bias: none
half_precision: true
flash_attention: true
input_emb_layer_norm_eps: 1.0e-05
input_emb_layer_dropout: 0.1
use_bos_token: true

#  Positional embedding params
pos_embedding_type: rope  # "pos", "rope"
init_pos_embed: "normal"   # "normal", "zero", "scaled"
use_patches_self_attention: false
max_num_embed: 24

# Encoder params
encoder_params:
    type: MLP
    num_layers: 2
    hidden_dim: 512
    activation: leakyrelu    # relu, leakyrelu, gelu, tanh, sigmoid, linear

# Decoder params
decoder_params:
    type: MLP
    num_layers: 2
    hidden_dim: 512
    activation: leakyrelu    # relu, leakyrelu, gelu, tanh, sigmoid, linear
    zero_last_layer: true

# Training params
batch_size: 8
num_epochs: 100
epoch_size: 100
learning_rate: 0.01
weight_decay: 0
optimizer: adam
loss_function: ['mse']
loss_weighting: [10]
schedule_epoch: 30
schedule_gamma: 0.75

# Dataloader params
multiprocess: true
num_workers: 8
load_dir: ./ds/MGN/cylinder_dataset
patch_size: [16, 16]
stride: [16, 16]
seq_len: 10
seq_interval: 2
resolution: 240

# Logging params
enable_wandb: false
save_model_each: 20
checkpoint_save_path: model_checkpoints


