task_name: cylinder_task

# LLM params
llm_backbone: facebook/opt-2.7b  # facebook/opt-350m, facebook/opt-125m, facebook/opt-2.7b, huggyllama/llama-7b
llm_layers: -1
llm_4bit_loading: false
freeze_llm: false
use_lora: true
lora_config:
  r: 16
  lora_alpha: 64
  lora_dropout: 0.1
  bias: none
  use_dora: True
half_precision: true
flash_attention: true
use_deepspeed: false
use_bos_token: true
see_init_state: true

# Training params
batch_size: 8
num_epochs: 501
optimizer: adamw
learning_rate: 10e-4
weight_decay: 1.0e-5
loss_function: ['mae', 'mse']
loss_weighting: [0.1, 10]
schedule_epoch: 50
schedule_gamma: 0.75

# Train modifications
pressure_weight: 0.1
diff_scale_factor: 0.05   # Scale factor for diffs
loss_norm_eps: 0.05
channel_independent: false
noise: null

#  Positional embedding params
pos_embedding_params:
  in_emb_ln_eps: null      # null to disable input layernorm
  input_emb_layer_dropout: 0.1    # null to disable input dropout
  pos_embedding_type: pos  # "pos", "rope"
  init_pos_embed: "normal"   # "normal", "zero", "scaled"

# Encoder params
encoder_params:
    type: MLP
    num_layers: 2
    hidden_dim: 512
    activation: leakyrelu    # relu, leakyrelu, gelu, tanh, sigmoid, linear

# Decoder params
decoder_params:
    type: MLPGNN
    gnn_dim: 16
    gnn_hid_dim: 32
    gnn_layers: 2
    gnn_heads: 2
    mlp_hid_dim: 512
    dropout: 0

# Teacher forcing params
teacher_forcing:
    tf_mode: "gen" # "gen", "notf"
    tf_prob: 0
    start_epoch: 10000

# Dataloader params
autoreg_seq_len: 10
# gen_seq_len: 10
val_seq_len: 26
num_workers: 6
load_dir: ./ds/MGN/cylinder_dataset
patch_size: [16, 16]
stride: [16, 16]
resolution: 238
normalize_ds: true
seq_interval: 1
seq_len: null

# Logging params
enable_wandb: false
save_on: false
save_model_each: 20
checkpoint_save_path: model_checkpoints
compile: true

